---
title: "Panel Regression Analysis"
subtitle: "MP223 - Applied Econometrics Methods for the Social Sciences"
author: "Eduard Bukin"
footer:  "[https://ebukin.github.io/mp223-2022-aem-R-public/](https://ebukin.github.io/mp223-2022-aem-R-public/)"
logo: "../../img/jlu-logo.png"
editor: visual
format: 
  revealjs: 
    
    transition: fade
    slide-number: true
    smaller: false
    scrollable: true
    incremental: true 
execute:
  
  freeze: auto
bibliography: ../../references.bib
editor_options: 
  chunk_output_type: console
---

# R setup {.smaller}

```{r}
#| echo: true

library(tidyverse)       # for data wrangling
library(alr4)            # for the data sets #
library(GGally)
library(parameters)
library(performance)
library(see)
library(car)
library(broom)
library(modelsummary)
library(texreg)
library(insight)
library(scales)
library(glue)

ggplot2::theme_set(ggplot2::theme_bw())

knitr::opts_chunk$set(
  fig.width = 10,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "100%", 
  message = FALSE,
  echo = TRUE, 
  cache = TRUE
)

# Custom functions to summaries data nicely
get_signif <- 
  function(x) {
    symnum(
      x,
      corr = FALSE,
      na = FALSE,
      cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
      symbols = c("***", "**", "*", ".", " ")
    ) %>% 
      as.character()
  }

tidy_skim <- 
  function(dta) {
    dta %>%
      select(- any_of(c("id", "time"))) %>% 
      skimr::skim_without_charts() %>%
      as_tibble() %>%
      select(any_of(c("skim_variable","n_missing")), contains("numeric")) %>%
      rename_with( ~ str_remove(., "numeric\\."))
  }

tidy_coeftest <- 
  function(
    mod, 
    mod_name = deparse(substitute(mod)), 
    mod_vcov = vcov(mod),
    dig = 3, 
    ...) {
    mod_name_sym <- sym(mod_name)
    mod %>%
      lmtest::coeftest(vcov. = mod_vcov)  %>%
      broom::tidy() %>%
      mutate(
          across(c(estimate, std.error),
                 ~ scales::number(., 1 / 10 ^ dig, big.mark = ",")),
        across(c(p.value), ~ insight::format_p(., stars_only = TRUE)),
        mod_stat := glue::glue("{estimate}{p.value} ({std.error})")
      ) %>%
      select(parameter = term, !!mod_name_sym := mod_stat)
  }

tidy_gof <- 
  function(
    mod, 
    mod_name = deparse(substitute(mod)), 
    dig = 3, 
    ...) {
    mod_sum <- summary(mod)
    mod_sum <- mod_sum$fstatistic
    if (is.vector(mod_sum)) {
      df1 <- mod_sum[[2]]
      df2 <- mod_sum[[3]]
      df <- str_c(c(df1, df2), collapse = "; ")
    } else {
      df <- str_c(mod_sum$parameter, collapse = "; ")
    }
    mod %>%
      broom::glance() %>%
      {
        dta <- .
        if (!"logLik" %in% names(dta)) {
          dta <-
            mutate(dta, logLik = mod %>% stats::logLik() %>% as.numeric())
        }
        
        if (!"AIC" %in% names(dta)) {
          dta <- mutate(dta, AIC = mod %>% stats::AIC() %>% as.numeric())
        }
        
        if (!"BIC" %in% names(dta)) {
          dta <- mutate(dta, BIC = mod %>% stats::BIC() %>% as.numeric())
        }
        dta
      } %>%
      mutate(
        across(any_of(c("r.squared", "deviance", "adj.r.squared")), 
               ~ scales::number(., 1 / 10 ^ dig, big.mark = ",")),
        across(any_of(c("statistic", "logLik", "AIC", "BIC")),
               ~ scales::number(., 1, big.mark = ",")),
        `F Statistics (df)` =
          glue("{statistic}{get_signif(p.value)} ", "({df})"),
        nobs = scales::number(nobs, 1, big.mark = ",")
      ) %>%
      select(
        N = nobs,
        `R-sq. adj.` = adj.r.squared,
        `Log likelihood` = logLik,
        AIC,
        BIC,
        `F Statistics (df)`
      ) %>%
      pivot_longer(everything(), 
                   names_to = "parameter", 
                   values_to = mod_name)
  }


tidy_summary <-
  function(mod,
           mod_name = deparse(substitute(mod)),
           mod_vcov = vcov(mod),
           dig = 3,
           ...) {
    
    tidy_coeftest(mod,mod_name = mod_name, mod_vcov = mod_vcov, dig = dig) %>% 
      bind_rows(tidy_gof(mod, mod_name = mod_name, dig = dig))
  }


tidy_summary_list <-
  function(mod_list,
           mod_vcov = NULL,
           dig = 3,
           ...) {
    # browser()
    mod_list %>%
      list(., names(.), seq_along(.)) %>% 
      pmap(~ {
        vcov_here <- vcov(..1)
        if (!is.null(mod_vcov[[..3]]))
          vcov_here <- mod_vcov[[..3]]
        tidy_summary(
          mod = .x,
          mod_name = .y,
          mod_vcov = vcov_here,
          dig = dig
        )
      }) %>%
      reduce(full_join, by = "parameter")
  }

```

# Recap

-   Ceteris paribus!?

    -   Why multiple regression is "good"?
    -   What variables are important when establishing a causal effect of a treatment (key variable)?
    -   What if we do not have an important variable?

-   Selection bias = OVB! In multiple regression analysis.

    -   What does OVB to our regression estimates?
    -   Bias (inconsistency) of estimates!

# Data Types

## Cross-sectional data {.smaller}

|    ID    |    Y     |     X1      |     X2      |
|:--------:|:--------:|:-----------:|:-----------:|
|   $1$    | $y_{1}$  | $x^{1}_{1}$ | $x^{2}_{1}$ |
|   $2$    | $y_{2}$  | $x^{1}_{2}$ | $x^{2}_{2}$ |
|   $3$    | $y_{3}$  | $x^{1}_{3}$ | $x^{2}_{3}$ |
| $\vdots$ | $\vdots$ |  $\vdots$   |  $\vdots$   |
|   $N$    | $y_{N}$  | $x^{1}_{N}$ | $x^{1}_{N}$ |

Could be repeated multiple times, but in every repetition, there are different individuals.

## Panel data

-   table with data, where

-   **each individual** (cohort) is represented by **multiple observations** from **different time periods**.

-   sometimes, nested cohorts are possible too (region, individual, time).

## Panel data {.smaller}

|    ID    |   Time   |    Y     |      X1      |      X2      |
|:--------:|:--------:|:--------:|:------------:|:------------:|
|   $1$    |   $1$    | $y_{11}$ | $x^{1}_{11}$ | $x^{2}_{11}$ |
|   $1$    |   $2$    | $y_{12}$ | $x^{1}_{12}$ | $x^{2}_{12}$ |
|   $1$    |   $3$    | $y_{13}$ | $x^{1}_{13}$ | $x^{2}_{13}$ |
|   $2$    |   $2$    | $y_{22}$ | $x^{1}_{22}$ | $x^{2}_{22}$ |
|   $2$    |   $3$    | $y_{23}$ | $x^{1}_{23}$ | $x^{2}_{23}$ |
|   $3$    |   $1$    | $y_{31}$ | $x^{1}_{31}$ | $x^{2}_{31}$ |
|   $3$    |   $2$    | $y_{32}$ | $x^{1}_{32}$ | $x^{2}_{32}$ |
| $\vdots$ | $\vdots$ | $\vdots$ |   $\vdots$   |   $\vdots$   |
|   $N$    |   $1$    | $y_{N1}$ | $x^{1}_{N1}$ | $x^{1}_{N1}$ |
|   $N$    |   $2$    | $y_{N2}$ | $x^{1}_{N2}$ | $x^{2}_{N2}$ |

## Panel data: Balanced and Unbalanced {.smaller}

|             |               |          |          |             |               |          |          |
|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|
|  Balanced   |               |          |          | Unbalanced  |               |          |          |
| $\text{ID}$ | $\text{Time}$ |   $Y$    |   $X$    | $\text{ID}$ | $\text{Time}$ |   $Y$    |   $X$    |
|      1      |       1       | $Y_{11}$ | $X_{11}$ |      1      |       1       | $Y_{11}$ | $X_{11}$ |
|      1      |       2       | $Y_{12}$ | $X_{12}$ |      1      |       2       | $Y_{12}$ | $X_{12}$ |
|      1      |       3       | $Y_{13}$ | $X_{13}$ |      2      |       2       | $Y_{22}$ | $X_{22}$ |
|      2      |       1       | $Y_{21}$ | $X_{21}$ |      2      |       3       | $Y_{23}$ | $X_{23}$ |
|      2      |       2       | $Y_{22}$ | $X_{22}$ |      3      |       3       | $Y_{33}$ | $X_{33}$ |
|      2      |       3       | $Y_{23}$ | $X_{23}$ |      4      |       1       | $Y_{31}$ | $X_{31}$ |

# Using Panel Data ...

::: callout-important
is a strategy to **control** unobserved/omitted but fixed effects using **time** or **cohort (individual)** dimensions.
:::

# Motivation for panel data regression analysis

## Example 1: Effect of an employee's union membership on wage {.smaller}

Does the collective bargaining (union membership) has any effect on wages?

-   See the following seminal papers: [@Freeman1984; @Card1996]

. . .

$$log(\text{Wage}_{it}) = \beta_0 + \beta_1 \cdot \text{Union}_{it} + \beta_2 \cdot {X_{it}} + \beta_3 \cdot \text{Ability}_{i} + \epsilon_{it}$$

where $i$ is the individual and $t$ is the time dimension;

## Is there an endogeneity problem?

-   Is there a source of endogeneity / selection bias here?

    -   Any ideas?
    -   Any ideas....

-   Ability:

    -   not observable ;
    -   time invariant;
    -   correlates with $X$ and $Y$;

-   Omitting ability causes the OVB!

## Solution: use the panel data

-   Most of the individual-related characteristics change over time: wage. union membership, skills, experience.

    -   These variables will be different each time we record measurements for each individual.

-   Ability are **time-invariant and specific to each individual**;

    -   If we introduce dummy variables for each individual,
    -   we can approximate different ability levels!

## Cross-sectional data and individual dummies

-   Can we introduce dummy variables for each individual in a cross-section?

    -   Any ideas?
    -   Why?....

-   NO...

    -   Because the **number of independent variables have to be less or equal to the number of observations**.

## Cross-sectional example

|    ID    |    Y     |     X1      |     X2      | ${ID}_1$ | ${ID}_2$ | ${ID}_3$ | ${ID}_N$ |
|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|
|   $1$    | $y_{1}$  | $x^{1}_{1}$ | $x^{2}_{1}$ |    1     |    0     |    0     |    0     |
|   $2$    | $y_{2}$  | $x^{1}_{2}$ | $x^{2}_{2}$ |    0     |    1     |    0     |    0     |
|   $3$    | $y_{3}$  | $x^{1}_{3}$ | $x^{2}_{3}$ |    0     |    0     |    1     |    0     |
| $\vdots$ | $\vdots$ |  $\vdots$   |  $\vdots$   | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
|   $N$    | $y_{N}$  | $x^{1}_{N}$ | $x^{1}_{N}$ |    0     |    0     |    0     |    1     |

## Panel data data and individual dummies {.smaller}

|    ID    |   Time   |    Y     |      X1      |      X2      | ${ID}_1$ | ${ID}_2$ | ${ID}_N$ |
|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|
|   $1$    |   $1$    | $y_{11}$ | $x^{1}_{11}$ | $x^{2}_{11}$ |    1     |    0     |    0     |
|   $1$    |   $2$    | $y_{12}$ | $x^{1}_{12}$ | $x^{2}_{12}$ |    1     |    0     |    0     |
|   $1$    |   $3$    | $y_{13}$ | $x^{1}_{13}$ | $x^{2}_{13}$ |    1     |    0     |    0     |
|   $2$    |   $2$    | $y_{22}$ | $x^{1}_{22}$ | $x^{2}_{22}$ |    0     |    1     |    0     |
|   $2$    |   $3$    | $y_{23}$ | $x^{1}_{23}$ | $x^{2}_{23}$ |    0     |    1     |    0     |
| $\vdots$ | $\vdots$ | $\vdots$ |   $\vdots$   |   $\vdots$   | $\vdots$ | $\vdots$ | $\vdots$ |
|   $N$    |   $1$    | $y_{N1}$ | $x^{1}_{N1}$ | $x^{1}_{N1}$ |    0     |    0     |    1     |
|   $N$    |   $2$    | $y_{N2}$ | $x^{1}_{N2}$ | $x^{2}_{N2}$ |    0     |    0     |    1     |

## With the panel data it will work, but... {.smaller}

May be difficult... Any ideas why?...

-   Number of dummy variables is equal to the number of individuals.

    -   If we have 5,000 individuals, we have 5,000 regression coefficients.
    -   What if we have 100,000 individuals?

-   Having too many regressors remains unbiased, but complicates inference:

    -   number of degrees of freedom increases;
    -   adjusted $R^2$ may shrink to zero;

# Panel regression: brief theory

## Readings {.smaller}

. . .

#### Key readings:

-   @Mundlak1961
-   @Angrist2009 Ch. 5
-   @Wooldridge2010;
-   @wooldridge2020introductory;
-   @Soederbom2014, Ch. 9-11

. . .

#### Other readings:

-   @Croissant2018

## Terminology:

-   Pooled OLS (regression without any panel structure);

-   Fixed Effect Models:

    -   Least-squares dummy variable (Pooled OLS + individual dummies);
    -   Within-transformation
    -   First-difference
    -   Between transformation (look it up in [@Croissant2018]!)

-   Random Effect Model

## Pooled OLS

-   Union example, the short model:

. . . $$log(\text{Wage}_{it}) = \beta_0 + \beta_1 \cdot \text{Union}_{it}  + \beta_2 \cdot X_{it} + \epsilon_{it}$$

-   Estimates are biased because we do not observe ability, which are time-invariant [@Mundlak1961].

## Least-squares dummy variable approach

-   Introduce a vector of dummy variables $\color{Red}{\delta}$

. . .

$$log(\text{Wage}_{it}) = \beta_0 + \beta_1 \cdot \text{Union}_{it}  + \beta_2 \cdot X_{it} + \\ \beta_3 \cdot \color{Red}{\delta_{i}} + \epsilon_{it}$$

-   Estimates and unbiased (consistent) but inefficient.

## Within transformation - Key method

. . .

$$log(\text{Wage}_{it} - \overline{\text{Wage}_{i}}) = \\ \beta_0 + \beta_1 \cdot (\text{Union}_{it} -  \overline{\text{Union}_{i}}) + \\ \beta_2 \cdot (X_{it} - \overline{\text{X}_{i}}) +  \beta_3 \cdot (\text{Ability}_{i} - \overline{\text{Ability}_{i}}) + \\ (\epsilon_{it} - \overline{\epsilon_{i}})$$

-   Any time-invariant effect will disappear from the regression because: $\text{Ability}_{i} - \overline{\text{Ability}_{i}} = 0$

-   Estimates are identical to the least-squares dummy variable, but SE are more efficient;

------------------------------------------------------------------------

::: smaller
|    ID    |   Time   |   $Y$    |     $Y-\overline{Y}$      |     $X1$     |      $X1-\overline{X1}$       |
|:----------:|:----------:|:----------:|:----------:|:----------:|:-------------:|
|   $1$    |   $1$    | $y_{11}$ | $y_{11} - \overline{y_1}$ | $x^{1}_{11}$ | $x^1_{11} - \overline{x^1_1}$ |
|   $1$    |   $2$    | $y_{12}$ | $y_{12} - \overline{y_1}$ | $x^{1}_{12}$ | $x^1_{12} - \overline{x^1_1}$ |
|   $1$    |   $3$    | $y_{13}$ | $y_{13} - \overline{y_1}$ | $x^{1}_{13}$ | $x^1_{13} - \overline{x^1_1}$ |
|   $2$    |   $2$    | $y_{22}$ | $y_{22} - \overline{y_2}$ | $x^{1}_{22}$ | $x^1_{22} - \overline{x^1_2}$ |
|   $2$    |   $3$    | $y_{23}$ | $y_{23} - \overline{y_2}$ | $x^{1}_{23}$ | $x^1_{23} - \overline{x^1_2}$ |
| $\vdots$ | $\vdots$ | $\vdots$ |         $\vdots$          |   $\vdots$   |           $\vdots$            |
|   $N$    |   $1$    | $y_{N1}$ | $y_{N1} - \overline{y_N}$ | $x^{1}_{N1}$ | $x^1_{N1} - \overline{x^1_N}$ |
|   $N$    |   $2$    | $y_{N2}$ | $y_{N2} - \overline{y_N}$ | $x^{1}_{N2}$ | $x^1_{N2} - \overline{x^1_N}$ |
:::

## First Difference transformation

. . .

$$log(\text{Wage}_{it} - {\text{Wage}_{i,t-1}}) = \\ \beta_0 + \beta_1 \cdot (\text{Union}_{it} -  {\text{Union}_{it-1}}) + \\ \beta_2 \cdot (X_{it} - {{X}_{i,t-1}}) + \\ \beta_3 \cdot (\text{Ability}_{i} - {\text{Ability}_{i,t-1}}) + (\epsilon_{it} - {\epsilon_{i,t-1}})$$

-   Has similar effect as the within transformation.
-   Sacrifices at least one time dimension.
-   Relaxes autocorrelation assumption.
-   May be not possible with unbalanced data.

## Fixed effect model assumptions {.smaller}

. . .

#### Very Important:

-   NOT ZERO CORRELATION between effects and regressors: $Cov(\delta_{i},{X}_{it}) \neq 0$

-   Strict exogeneity: $E[\epsilon_{is}| {X}_{it}, \delta_{i}] = 0$

    -   $Cov(\epsilon_{is}, {X}_{jt}) = 0$ and $Cov(\epsilon_{it}, {X}_{it}) = 0$ , where $j\neq i$ and $s\neq t$ ;

    -   Residuals ($\epsilon$) do not correlate with all explanatory variable ($X$) in all time periods ($t$) and for all individuals ($i$).

        -   No autocorrelation/serial correlation: $Cov(\epsilon_{it}, {X}_{i,t-1}) = 0$;
        -   No cross-sectional dependence: $Cov(\epsilon_{it}, {X}_{j,t}) = 0$ (when individual observations react similarly to the common shocks or correlate in space);

. . .

#### Not less important:

-   Linearity
-   Homoscedasticity of error terms: $Var(\delta_{i}|{X}_{it}) = \sigma^2_{\delta}$

## Fixed effect: literature {.smaller}

-   Seminal papers: @Mundlak1961
-   Climate and agriculture: @Mendelsohn1994, @Blanc2017, @Bozzola2017
-   Choice of irrigation: @Kurukulasuriya2011, @Chatzopoulos2015
-   Crop choice: @Kurukulasuriya2008, @Seo2008b,
-   Livestock choice @Seo2008a, @Seo2008
-   Cross-sectional dependence: @Conley1999

## Random Effect Model {.smaller}

-   Introduce a random component of the error term $\color{Red}{v}$

. . .

$$log(\text{Wage}_{it}) = \beta_0 + \beta_1 \cdot \text{Union}_{it}  + \beta_2 \cdot X_{it} + \beta_3 \cdot \color{Red}{v_{i}} + \epsilon_{it}$$

-   Difference from the fixed effect model:

    -   Assumes NO CORRELATION (ZERO CORRELATION) between effects and regressors: $Cov(v_{i},{X}_{it}) = 0$.

    -   Ignoring RE causes no bias to the estimates;

## Limitations of Fixed and Random effect models

-   NOT the ultimate solution to Endogeneity.

-   There might still be some OVB even with the fixed effects.

    -   Instrumental Variables are possible within the panel regression context too.

-   Measurement error may cause endogeneity;

## Example 1. Unions premium: implication of the Fixed effect model:

```{r}
#| echo: false
knitr::include_graphics("./img/union-wage-tbl.png")
```

::: footer
Source: @Angrist2009
:::

# Panel regression: Empirical motivation

## Example 2. Macro-level data analysis {.smaller}

-   Let us analyze a link between imports and national product based on [@Kinal1993].
-   Both variables are in per capita and in log.

```{r}
library(plm); library(pder); library(splm)
data("ForeignTrade", package = "pder")
ForeignTrade <- ForeignTrade %>% 
  select(country, year, exports, imports, gnp) %>% 
  pdata.frame(index = c("country", "year"))
pdim(ForeignTrade)
head(ForeignTrade)
```

------------------------------------------------------------------------

### Relationship between GNP and imports

```{r}
#| echo: false
#| fig-width: 8
pp1 <- 
  ForeignTrade %>% 
  as_tibble() %>% 
  ggplot() + 
  aes(x = gnp, y = imports) + 
  geom_point(aes(color = country), alpha = 0.5) + 
  xlab("Real GNP per capita") + 
  ylab("Imports deflated by the unit value of exports per capita")
pp1
```

------------------------------------------------------------------------

### Pooled

```{r}
#| echo: false
#| message: false
#| fig-width: 8
fit1_pooled <- plm(imports ~ gnp, ForeignTrade, model = "pooling")
fit1_within <- plm(imports ~ gnp, ForeignTrade, model = "within", effect = "individual")
# fit1_between <- plm(imports ~ gnp, ForeignTrade, model = "between", effect = "individual")
fit1_within_two <- plm(imports ~ gnp, ForeignTrade, model = "within", effect = "twoways")
fit1_random <- plm(imports ~ gnp, ForeignTrade, model = "random", effect = "individual")
fttd_dta <- 
  ForeignTrade %>% 
  mutate(ftt = fitted(fit1_within) %>% as.numeric(),
         res = resid(fit1_within) %>% as.numeric(),
         ftt = imports - res) %>% 
  as_tibble()
pp1
```

------------------------------------------------------------------------

### Pooled

```{r}
#| echo: false
#| message: false
#| fig-width: 8
pp1 +
  guides(colour = "none") + 
  geom_abline(
    aes(slope = coef(fit1_pooled)[[2]],
        intercept = coef(fit1_pooled)[[1]],
        linetype = "Pooled OLS"), size = 1) 
```

------------------------------------------------------------------------

### Pooled + Within

```{r}
#| echo: false
#| message: false
#| fig-width: 8
pp1 +
  guides(colour = "none") + 
  geom_abline(
    aes(slope = coef(fit1_pooled)[[2]],
        intercept = coef(fit1_pooled)[[1]],
        linetype = "Pooled OLS"), size = 1) + 
  geom_abline(
    aes(slope = coef(fit1_within)[[1]],
        intercept = within_intercept(fit1_within),
        linetype = "Within"), size = 1)
```

------------------------------------------------------------------------

### Pooled + Within + Random

```{r}
#| echo: false
#| message: false
#| fig-width: 8
pp1 +
  guides(colour = "none") + 
  geom_abline(
    aes(slope = coef(fit1_pooled)[[2]],
        intercept = coef(fit1_pooled)[[1]],
        linetype = "Pooled OLS"), size = 1) + 
  geom_abline(
    aes(slope = coef(fit1_within)[[1]],
        intercept = within_intercept(fit1_within),
        linetype = "Within"), size = 1) + 
  geom_abline(
    aes(slope = coef(fit1_random)[[2]],
        intercept = coef(fit1_random)[[1]],
        linetype = "Random"), size = 1) + 
  geom_path(
    data = fttd_dta,
    aes(x = gnp, y = ftt, color = country)
  )

  # geom_abline(
  #   aes(slope = coef(fit1_within_two)[[1]],
  #       intercept = within_intercept(fit1_within_two),
  #       linetype = "Within + two-ways"), size = 1) + 
  # geom_abline(
  #   aes(slope = coef(fit1_between)[[2]],
  #       intercept = coef(fit1_between)[[1]],
  #       linetype = "between"), size = 1) + 
  # guides(colour = "none") + 
  # geom_smooth(method = "lm", formula = "y ~ x", 
  #             se = FALSE, inherit.aes = T, 
  #             aes(group = country))+ 
```

# Panel regression: Empirical practice

## General algorithm {.smaller}

1.  Pooled OLS using;

    -   Gauss-Markov assumption validation: Linearity, Collinearity, No endogeneity, Homoscedasticity, validation;

2.  FE: Fixed Effect. Within-transformation. Individual, Time or Two-ways effects;

    -   `F-test` and `LM test` on FE consistency against pooled;

3.  RE: Random Effect;

    -   `Hausman test`, `Chamberlain test`, `Angrist and Newey` on effects' correlation with regressors of RE consistency against the FE;

4.  Choosing the appropriate functional form;

5.  Serial correlation and cross-sectional dependence tests;

6.  Robust standard errors:

    -   Clustered SE and/or heteroscedasticity and/or autocorrelation robust SE;

7.  Summary and interpretation;

# Example 3. Micro-level application `RiceFarms`

-   Let us explore the determinants of rice farms productivity.

. . .

::: callout-tip
We want to understand if larger farms are more productive compared to smaller once.
:::

## Farm-specific Rice production function (1) {.smaller}

-   We employ a production function approach, relying on the Cobb-Douglas Production function:

. . .

$$\ln y = \ln \beta_0 + \sum_{n = 1}^{N}  \beta_n \ln x_n + \sum_{k = 1}^{K} \gamma_k \delta_k + \epsilon$$

where,

-   $y$ is the output and $x_n$ are the inputs all in physical mass (or monetary value); $N$ is the number of independent variables;
-   $\delta_k$ are the shift parameters of additional dummy variables;
-   $\beta_0$ , $\beta_n$ , $\gamma_n$ are the estimated coefficients;

## Farm-specific Rice production function (2) {.smaller}

-   Inputs are: land, seeds, urea, pesticide and labor;
-   Output: rice production in physical mass;
-   short model (all things are in log): . . .

$$\text{output} = A_0 + \beta_1 \cdot \text{land} + \beta_2 \cdot \text{labor} + \\ \beta_3 \cdot \text{seed} + \beta_4 \cdot \text{urea} + \beta_1 \cdot  \text{pesticide}$$

-   What are the ex-ante expectations about the regression coefficients?

    -   Ideas? ...

## What about OVB? (3) {.smaller}

-   What omitted variables could cause bias of the regression?

    -   Any!? ...
    -   Any!? ...
    -   Capital, Ability, Climate, Geography...

-   Let us speculate on the bias of land estimated given exclusion of ability.

    -   Long model.
    -   Short model.
    -   OVB formula.

-   Educated guess about bias of the estimates.

## Farm-level data

-   Following data `RiceFarms` is used from package `splm`. We only use a subset of variables:

    -   `output` - gross output of rice in kg
    -   `land` - the total area cultivated with rice, measured in hectares
    -   `seed` - seed in kilogram
    -   `urea` - urea in kilogram
    -   `pesticide` - urea in kilogram
    -   `labor` - total labor (excluding harvest labor)

-   We calculate logs of all variables.

-   we do not use logs for summary statistics.

## Data glimpse (1) {.smaller}

```{r}
data("RiceFarms", package = "splm")
rice_dta_selection <- RiceFarms %>% as_tibble() %>%
  select(
    id, time,
    output = goutput,
    land = size,
    labor = totlabor,
    seed, urea,
    pest = pesticide
  )

rice_dta_1 <- rice_dta_selection %>%
  mutate(across(c(output, land, seed, urea, pest, labor), ~ log(.)))

# Making panel structure
rice_dta_p1 <- rice_dta_1 %>% pdata.frame(index = c("id", "time"))
pdim(rice_dta_p1)
```

## Data glimpse (2) {.smaller}

```{r}
glimpse(rice_dta_1)
tidy_skim(rice_dta_1)
```

-   Any problems with data?

## Any problems with data? (1)

-   Any? ...

-   `pest`, when transformed with logs, produces `-Inf` values.

    -   Why is that so?
    -   Any? ...
    -   Because there are zero values of pesticides application $\ln 0 = - \infty$.

## `-Infinity` in logs: lazy solution {.smaller}

-   Before log transformation, substitute any zero with a small value, for example `0.0001`;

. . .

```{r}
rice_dta_lazy <- 
  rice_dta_selection %>% 
  mutate(pest = ifelse(pest <= 0, 0.0001, pest))%>%
  mutate(across(c(output, land, seed, urea, pest, labor), ~ log(.)))
rice_dta_p_lazy <- rice_dta_lazy %>% pdata.frame(index = c("id", "time"))
rice_dta_lazy %>% tidy_skim()
```

## `-Infinity` in logs: smart solution {.smaller}

-   Introduce reverse dummy variables for each variable with log of zero, see: @Battese1997;
-   Substitute negative infinity with zero.

. . .

```{r}
rice_dta <- 
  rice_dta_selection %>% 
  mutate(pest_revdum = ifelse(pest <= 0, 1, 0),
    across(c(output, land, seed, urea, pest, labor), ~ log(.)),
    pest = ifelse(is.infinite(pest), 0, pest)) 
rice_dta_p <- rice_dta %>% pdata.frame(index = c("id", "time"))
rice_dta %>% tidy_skim()
```

## Data exploration

```{r}
#| fig-asp: 0.5
library(GGally)
rice_dta %>% select(-id, -time) %>% ggpairs()
```

## Step 1.1 Pooled OLS

```{r}
rice_pooled <- 
  plm(output ~ land + labor + seed + urea + pest + pest_revdum, 
      rice_dta_p, model = "pooling")
rice_pooled_2 <-   
  lm(output ~ land + labor + seed + urea + pest + pest_revdum, rice_dta_p)
tidy_summary_list(list(pooled_plm = rice_pooled, pooled_lm = rice_pooled_2))
```

## Step 1.2 Linearity and homoscedasticity

```{r}
check_model(rice_pooled_2, check = c("linearity", "homogeneity"))
```

## Step 2.1 Fixed Effect

```{r}
rice_fe <- 
  plm(output ~ land + labor + seed + urea + pest + pest_revdum, 
      rice_dta_p, model = "within", effect = "individual")

tidy_summary_list(list(pooled = rice_pooled, FE = rice_fe))
```

## Step 2.1 lazy versus reverse dummy

```{r}
#| code-fold: true
rice_fe_lazy <- 
  plm(output ~ land + labor + seed + urea + pest, 
      rice_dta_p_lazy, model = "within", effect = "individual")
rice_pooled_lazy <- 
  plm(output ~ land + labor + seed + urea + pest, 
      rice_dta_p_lazy, model = "pooling")

list(pooled = rice_pooled, `pooled (lazy)` = rice_pooled_lazy, `FE (rev. dumy)` = rice_fe, `FE (lazy)` = rice_fe_lazy) %>% 
  tidy_summary_list()
```

## Step 2.2 F test for individual effects {.smaller}

-   Compares FE model to OLS. OLS is always consistent, when Gauss-Markov assumptions are satisfied.

    -   H0: One model is inconsistent.
    -   H1: Both models are equally consistent.

. . .

```{r}
pFtest(rice_fe, rice_pooled)
```

## Step 2.3 Lagrange Multiplier Tests {.smaller}

-   Compares FE model to OLS. OLS is always consistent, when Gauss-Markov assumptions are satisfied.

    -   H0: One model is inconsistent.
    -   H1: Both models are equally consistent.

. . .

```{r}
plmtest(rice_pooled, effect = "individual", type = "honda")
plmtest(rice_pooled, effect = "individual", type = "bp")
```

## Step 3.1 Random Effect

```{r}
rice_re <- 
  plm(output ~ land + labor + seed + urea + pest + pest_revdum, 
      rice_dta_p, model = "random", effect = "individual")
list(pooled = rice_pooled, FE = rice_fe, RE = rice_re) %>% 
  tidy_summary_list()
```

## Step 3.2 Hausman Test for Panel Models

-   Compares RE to FE model. FE is assumed to be consistent

    -   H0: One model is inconsistent.
    -   H1: Both models are equally consistent.

. . .

```{r}
phtest(rice_fe, rice_re)
```

-   Fixed Effect model is recommended

## Step 4.1 Serial correlation and cross-sectional dependence

-   Wooldridge's test for unobserved individual effects

    -   H0: no unobserved effects
    -   H1: some effects also dues to serial correlation

. . .

```{r}
pwtest(rice_pooled)
```

## Step 4.2 lm tests for random effects and/or serial correlation {.smaller}

-   H0: serial correlation is zero
-   H1: some serial correlation

. . .

```{r}
pbsytest(rice_pooled, test = "ar")
pbsytest(rice_pooled, test = "re")
```

## Step 4.3 Breusch-Godfrey and Durbin-Watson tests {.smaller}

-   H0: serial correlation is zero
-   H0: some serial correlation

. . .

```{r}
pbgtest(rice_fe)
pbgtest(rice_fe, order = 2)
pdwtest(rice_fe)
```

## Step 5. Robust and clustered standard errors (1) {.smaller}

```{r}
library(lmtest); library(car); library(sandwich); options(digits = 4, scipen = 15)
# Regular SE
vcov(rice_fe)
# Clustered, heteroscedasticity and autocorrelation robust SE
vcovHC(rice_fe, method = "arellano", type = "HC0", cluster = "group")
```

## Step 5. Robust and clustered standard errors (2)

```{r}
coeftest(rice_fe,
         vcov. = vcovHC(rice_fe, method = "arellano", 
                        type = "HC0", cluster = "group"))
```

## Step 5. Robust and clustered standard errors (2)

```{r}
#| code-fold: true
tidy_summary_list( 
  mod_list = 
    list(`FE regular SE` = rice_fe, 
         `FE cl. het. robust` = rice_fe, 
         `FE cl. het. autocor. robust` = rice_fe),
  mod_vcov = list(
    vcov(rice_fe),
    vcovHC(rice_fe, method = "white1", type = "HC0"),
    vcovHC(rice_fe, method = "arellano", type = "sss")
    )
)
```

# Are larger farms more productive?

::: callout-tip
Remember:

We want to understand if larger farms are more productive compared to smaller once.
:::

-   It is possible to understand by performing a hypothesis testing about a linear combination of parameters taking into consideration their covariance.

## Linear hypothesis (1) {.smaller}

```{r}
lh_1 <- 
  linearHypothesis(rice_fe,
                   "land + labor + seed + urea + pest = 1",
                   vcov. = vcovHC(rice_fe))
lh_1
# Sum of the linear combinations
attr(lh_1, "value") %>% as.vector() 

# SE of the linear combinations
attr(lh_1, "vcov") %>% as.vector()
```

## Linear hypothesis: delta method (2)

```{r}
deltaMethod(
  rice_fe, 
  "land + labor + seed + urea + pest",
  vcov. = vcovHC(rice_fe)
)
```

# Class/Homework

## Reproduce example 3 from the class {.smaller}

-   Instead of using a Cobb-Douglas production function, use the Translog production function.

-   Reduce number of regressors to land, labor and seeds for simplicity.

-   Compute the marginal effects of coefficients.

. . . $$\ln y = \beta_0 + \sum_{n = 1}^{N}  \beta_n \ln x_n + \\ \frac{1}{2} \sum_{n = 1}^{N} \sum_{m = 1}^{M}  \beta_{nm} \ln x_n \ln x_m + \sum_{k = 1}^{K} \gamma_k \delta_k + \epsilon$$

where,

-   $\ln x_n \ln x_m$ are the interaction terms between all combination of two regressors.
-   Everything else is the same as in Cobb-Douglas.

# Take away {.smaller}

## Take away {.smaller}

-   Data types: cross-sectional and panel balanced and unbalanced

-   Why panel data:

-   FE vs RE;

    -   Correlated and uncorrelated individual effects
    -   Limitations of the panel regression methods
    -   Within, first difference, Random effect

-   Practical application;

    -   Fitting panel regression
    -   Cobb-Douglas and Translog production function
    -   Model selection routine
    -   Statistical testing
    -   Standard Errors correction
    -   Results presentation

-   Linear hypothesis testing

    -   Linear combination of parameters
    -   Delta method

# References
